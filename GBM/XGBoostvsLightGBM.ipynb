XGBoost vs LightGBM
In this notebook we collect the results from all the experiments and reports the comparative difference between XGBoost and LightGBM

In [1]:
import matplotlib.pyplot as plt
import nbformat
import json
from toolz import pipe, juxt
import pandas as pd
import seaborn
from toolz import curry

from bokeh.io import show, output_notebook
from bokeh.charts import Bar
from bokeh.models.renderers import GlyphRenderer
from bokeh.models.glyphs import Rect
from bokeh.models import Range1d
from toolz import curry
from bokeh.io import export_svgs
from IPython.display import SVG, display
import warnings
warnings.filterwarnings("ignore")
%matplotlib inline
/anaconda/envs/strata/lib/python3.5/site-packages/bokeh/util/deprecation.py:34: BokehDeprecationWarning: 
The bokeh.charts API has moved to a separate 'bkcharts' package.

This compatibility shim will remain until Bokeh 1.0 is released.
After that, if you want to use this API you will have to install
the bkcharts package explicitly.

  warn(message)
In [2]:
output_notebook()
Loading BokehJS ...
We are going to read the results from the following notebooks

In [3]:
notebooks = {
    'Airline':'01_airline.ipynb',
    'Airline_GPU': '01_airline_GPU.ipynb',
    'BCI': '02_BCI.ipynb',
    'BCI_GPU': '02_BCI_GPU.ipynb',
    'Football': '03_football.ipynb',
    'Football_GPU': '03_football_GPU.ipynb',
    'Planet': '04_PlanetKaggle.ipynb',
    'Plannet_GPU': '04_PlanetKaggle_GPU.ipynb',
    'Fraud': '05_FraudDetection.ipynb',
    'Fraud_GPU': '05_FraudDetection_GPU.ipynb',
    'HIGGS': '06_HIGGS.ipynb',
    'HIGGS_GPU': '06_HIGGS_GPU.ipynb'
}
In [4]:
def read_notebook(notebook_name):
    with open(notebook_name) as f:
        return nbformat.read(f, as_version=4)
In [5]:
def results_cell_from(nb):
    for cell in nb.cells:
        if cell['cell_type']=='code' and cell['source'].startswith('# Results'):
            return cell
In [6]:
def extract_text(cell):
    return cell['outputs'][0]['text']
In [7]:
@curry
def remove_line_with(match_str, json_string):
    return '\n'.join(filter(lambda x: match_str not in x, json_string.split('\n')))
In [8]:
def process_nb(notebook_name):
    return pipe(notebook_name,
                read_notebook,
                results_cell_from,
                extract_text,
                remove_line_with('total RAM usage'),
                json.loads)
Here we collect the results from all the exeperiment notebooks. The method simply searches the notebooks for a cell that starts with # Results. It then reads that cells output in as JSON.

In [9]:
results = {nb_key:process_nb(nb_name) for nb_key, nb_name in notebooks.items()}
In [10]:
results
Out[10]:
{'Airline': {'lgbm': {'performance': {'AUC': 0.8402372555827792,
    'Accuracy': 0.7643602509172321,
    'F1': 0.7327938097266504,
    'Precision': 0.7934939262260393,
    'Recall': 0.6807205483883755},
   'test_time': 45.19994992000284,
   'train_time': 1056.2073636010027},
  'xgb_hist': {'performance': {'AUC': 0.8384000721790869,
    'Accuracy': 0.7163687932685965,
    'F1': 0.7414381273804327,
    'Precision': 0.6534895643276287,
    'Recall': 0.856740851230127},
   'test_time': 36.26451232100226,
   'train_time': 1242.098871958995}},
 'Airline_GPU': {'lgbm': {'performance': {'AUC': 0.8420633400407397,
    'Accuracy': 0.7253679574224235,
    'F1': 0.7459552507319687,
    'Precision': 0.6647341223976702,
    'Recall': 0.8497872609318392},
   'test_time': 48.380596682982286,
   'train_time': 645.403558799997},
  'xgb_hist': {'performance': {'AUC': 0.8411899192614771,
    'Accuracy': 0.7245983323747782,
    'F1': 0.745273441487613,
    'Precision': 0.6640651298572008,
    'Recall': 0.8491109466751164},
   'test_time': 31.72845547500765,
   'train_time': 1271.9167441620084}},
 'BCI': {'lgbm': {'performance': {'AUC': 0.5348626373626374,
    'Accuracy': 0.8802275008364001,
    'F1': 0.13734939759036147,
    'Precision': 0.5588235294117647,
    'Recall': 0.0782967032967033},
   'test_time': 0.19060174500009452,
   'train_time': 7.311910866001199},
  'xgb': {'performance': {'AUC': 0.5277435897435897,
    'Accuracy': 0.8812311809969889,
    'F1': 0.11027568922305765,
    'Precision': 0.6285714285714286,
    'Recall': 0.06043956043956044},
   'test_time': 0.2988008609991084,
   'train_time': 11.515288856999177},
  'xgb_hist': {'performance': {'AUC': 0.536521978021978,
    'Accuracy': 0.8810639009702241,
    'F1': 0.14234016887816647,
    'Precision': 0.5841584158415841,
    'Recall': 0.08104395604395605},
   'test_time': 0.4702643149994401,
   'train_time': 41.84106117400006}},
 'BCI_GPU': {'lgbm': {'performance': {'AUC': 0.7714542124542124,
    'Accuracy': 0.8813984610237537,
    'F1': 0.13851761846901578,
    'Precision': 0.6,
    'Recall': 0.0782967032967033},
   'test_time': 0.009907090001433971,
   'train_time': 2.7659428379993187},
  'xgb': {'performance': {'AUC': 0.7716584249084248,
    'Accuracy': 0.8798929407828705,
    'F1': 0.09343434343434343,
    'Precision': 0.578125,
    'Recall': 0.050824175824175824},
   'test_time': 0.0064387769998575095,
   'train_time': 12.934047714998997},
  'xgb_hist': {'performance': {'AUC': 0.7736170852956569,
    'Accuracy': 0.8805620608899297,
    'F1': 0.12068965517241378,
    'Precision': 0.5833333333333334,
    'Recall': 0.0673076923076923},
   'test_time': 0.00308577800024068,
   'train_time': 42.69890288699935}},
 'Football': {'lgbm': {'performance': {'Accuracy': 0.5273189326556544,
    'F1': 0.4594968503939465,
    'Precision': 0.4627647374365573,
    'Recall': 0.5273189326556544},
   'test_time': 0.01769829506520182,
   'train_time': 0.5824753509368747},
  'xgb': {'performance': {'Accuracy': 0.5285895806861499,
    'F1': 0.4581501758019101,
    'Precision': 0.4728104942149788,
    'Recall': 0.5285895806861499},
   'test_time': 0.01739849301520735,
   'train_time': 2.277035239036195},
  'xgb_hist': {'performance': {'Accuracy': 0.5285895806861499,
    'F1': 0.46087114582653593,
    'Precision': 0.4747655220586142,
    'Recall': 0.5285895806861499},
   'test_time': 0.013859358034096658,
   'train_time': 2.4757352949818596}},
 'Football_GPU': {'lgbm': {'performance': {'Accuracy': 0.5344345616264294,
    'F1': 0.4704311590503636,
    'Precision': 0.48847893806298454,
    'Recall': 0.5344345616264294},
   'test_time': 0.029374134999670787,
   'train_time': 0.976751588001207},
  'xgb': {'performance': {'Accuracy': 0.5359593392630242,
    'F1': 0.4704659043141339,
    'Precision': 0.4825747269523364,
    'Recall': 0.5359593392630242},
   'test_time': 0.006567717999132583,
   'train_time': 7.09927419500309},
  'xgb_hist': {'performance': {'Accuracy': 0.537992376111817,
    'F1': 0.4723094570741036,
    'Precision': 0.4944404394401915,
    'Recall': 0.537992376111817},
   'test_time': 0.007724854996922659,
   'train_time': 4.588017762001982}},
 'Fraud': {'lgbm': {'performance': {'AUC': 0.8749179318834633,
    'Accuracy': 0.999403110845827,
    'F1': 0.8131868131868133,
    'Precision': 0.888,
    'Recall': 0.75},
   'test_time': 0.05075380699963716,
   'train_time': 0.6608378439996159},
  'xgb': {'performance': {'AUC': 0.8884197213803287,
    'Accuracy': 0.9994265182636377,
    'F1': 0.8243727598566308,
    'Precision': 0.8778625954198473,
    'Recall': 0.777027027027027},
   'test_time': 0.06871192899961898,
   'train_time': 4.349258283999916},
  'xgb_hist': {'performance': {'AUC': 0.8715278294884368,
    'Accuracy': 0.9993679997191109,
    'F1': 0.8029197080291971,
    'Precision': 0.873015873015873,
    'Recall': 0.7432432432432432},
   'test_time': 0.08524090300034004,
   'train_time': 2.0142575339996256}},
 'Fraud_GPU': {'lgbm': {'performance': {'AUC': 0.96572630283135,
    'Accuracy': 0.9993914071369217,
    'F1': 0.8115942028985507,
    'Precision': 0.875,
    'Recall': 0.7567567567567568},
   'test_time': 0.04103280100025586,
   'train_time': 0.2938522020012897},
  'xgb': {'performance': {'AUC': 0.9699805365480376,
    'Accuracy': 0.9994148145547324,
    'F1': 0.8214285714285714,
    'Precision': 0.8712121212121212,
    'Recall': 0.777027027027027},
   'test_time': 0.03786925599706592,
   'train_time': 5.8025254829990445},
  'xgb_hist': {'performance': {'AUC': 0.9692088902901377,
    'Accuracy': 0.9994148145547324,
    'F1': 0.8148148148148148,
    'Precision': 0.9016393442622951,
    'Recall': 0.7432432432432432},
   'test_time': 0.00477967400001944,
   'train_time': 1.6418742589994508}},
 'HIGGS': {'lgbm': {'performance': {'AUC': 0.694682949690134,
    'Accuracy': 0.707758,
    'F1': 0.7680747894958216,
    'Precision': 0.6627597069095391,
    'Recall': 0.9131831219806763},
   'test_time': 0.7120589099995414,
   'train_time': 119.34003880199998},
  'xgb': {'performance': {'AUC': 0.6859901403358623,
    'Accuracy': 0.699694,
    'F1': 0.7635493812093622,
    'Precision': 0.6551156676187414,
    'Recall': 0.9149984903381643},
   'test_time': 0.55617916600022,
   'train_time': 2996.1667750769993},
  'xgb_hist': {'performance': {'AUC': 0.6941216899970567,
    'Accuracy': 0.70721,
    'F1': 0.767674555527519,
    'Precision': 0.6623426413523601,
    'Recall': 0.9128434480676328},
   'test_time': 0.6464068210007099,
   'train_time': 121.21175534400027}},
 'HIGGS_GPU': {'lgbm': {'performance': {'AUC': 0.8206713959277258,
    'Accuracy': 0.707346,
    'F1': 0.7678274211385622,
    'Precision': 0.6623814985860588,
    'Recall': 0.9132019927536232},
   'test_time': 0.6611301500006448,
   'train_time': 71.8760530440004},
  'xgb_hist': {'performance': {'AUC': 0.8205886356415744,
    'Accuracy': 0.70721,
    'F1': 0.767674555527519,
    'Precision': 0.6623426413523601,
    'Recall': 0.9128434480676328},
   'test_time': 0.5808831149997786,
   'train_time': 114.88390647300002}},
 'Planet': {'lgbm': {'performance': {'Accuracy': 0.37233071728417594,
    'F1': 0.822258366139549,
    'Precision': 0.7439077632634851,
    'Recall': 0.9734099462015139},
   'test_time': 0.1641630920021271,
   'train_time': 194.57900593099475},
  'xgb': {'performance': {'Accuracy': 0.34057309728052565,
    'F1': 0.8048263053953228,
    'Precision': 0.7184218531362171,
    'Recall': 0.9766441564762427},
   'test_time': 0.1852665030019125,
   'train_time': 313.8951129560046},
  'xgb_hist': {'performance': {'Accuracy': 0.37871874429640445,
    'F1': 0.8220252909027159,
    'Precision': 0.7447899193746976,
    'Recall': 0.9720717197264013},
   'test_time': 0.19687007299944526,
   'train_time': 2115.2851170680005}},
 'Plannet_GPU': {'lgbm': {'performance': {'Accuracy': 0.3719656871691915,
    'F1': 0.8219281534713183,
    'Precision': 0.7435648956911961,
    'Recall': 0.9733034790846435},
   'test_time': 0.2703422480117297,
   'train_time': 317.68381078500533},
  'xgb_hist': {'performance': {'Accuracy': 0.37871874429640445,
    'F1': 0.8220252909027159,
    'Precision': 0.7447899193746976,
    'Recall': 0.9720717197264013},
   'test_time': 0.24103524297242984,
   'train_time': 2028.4320792920043}}}
In [11]:
datasets = [k for k in results.keys()]
print(datasets)
algos = [a for a in results[datasets[0]].keys()]
print(algos)
['Football_GPU', 'Airline_GPU', 'Planet', 'Airline', 'HIGGS_GPU', 'HIGGS', 'BCI_GPU', 'Plannet_GPU', 'Football', 'Fraud', 'Fraud_GPU', 'BCI']
['xgb', 'lgbm', 'xgb_hist']
We wish to compare LightGBM and XGBoost both in terms of performance as well as how long they took to train.

In [12]:
def average_performance_diff(dataset):
    lgbm_series = pd.Series(dataset['lgbm']['performance'])
    try:
        perf = 100*((lgbm_series-pd.Series(dataset['xgb']['performance']))/lgbm_series).mean()
    except KeyError:
        perf = None
    return perf
In [13]:
def train_time_ratio(dataset):
    try: 
        val = dataset['xgb']['train_time']/dataset['lgbm']['train_time']
    except KeyError:
        val = None
    return val

def train_time_ratio_hist(dataset):
    try: 
        val = dataset['xgb_hist']['train_time']/dataset['lgbm']['train_time']
    except KeyError:
        val = None
    return val

def test_time_ratio(dataset):
    try: 
        val = dataset['xgb']['test_time']/dataset['lgbm']['test_time']
    except KeyError:
        val = None
    return val
In [14]:
metrics = juxt(average_performance_diff, train_time_ratio, train_time_ratio_hist, test_time_ratio)
res_per_dataset = {dataset_key:metrics(dataset) for dataset_key, dataset in results.items()}
In [15]:
results_df = pd.DataFrame(res_per_dataset, index=['Perf. Difference(%)', 
                                                  'Train Time Ratio',
                                                  'Train Time Ratio Hist',
                                                  'Test Time Ratio']).T
In [16]:
results_df
Out[16]:
Perf. Difference(%)	Train Time Ratio	Train Time Ratio Hist	Test Time Ratio
Airline	NaN	NaN	1.175999	NaN
Airline_GPU	NaN	NaN	1.970731	NaN
BCI	6.250871	1.574867	5.722316	1.567671
BCI_GPU	14.284971	4.676180	15.437377	0.649916
Football	-0.589916	3.909239	4.250369	0.983060
Football_GPU	0.157673	7.268249	4.697221	0.223588
Fraud	-1.076624	6.581430	3.048036	1.353828
Fraud_GPU	-0.780054	19.746408	5.587415	0.922902
HIGGS	0.786893	25.106132	1.015684	0.781086
HIGGS_GPU	NaN	NaN	1.598361	NaN
Planet	3.435782	1.613201	10.871086	1.128551
Plannet_GPU	NaN	NaN	6.385066	NaN
In [17]:
results_gpu = results_df.ix[[idx for idx in results_df.index if idx.endswith('GPU')]]
results_cpu = results_df.ix[~results_df.index.isin(results_gpu.index)]
Plot of train time ratio for CPU experiments.

In [18]:
data = {
    'Ratio': results_cpu['Train Time Ratio'].values.tolist() + results_cpu['Train Time Ratio Hist'].values.tolist(),
    'label': results_cpu.index.values.tolist()*2,
    'group': ['xgb/lgb']*len(results_cpu.index.values) + ['xgb_hist/lgb']*len(results_cpu.index.values)
}
In [19]:
bar = Bar(data, values='Ratio', agg='mean', label='label', group='group', 
          plot_width=600, plot_height=400, bar_width=0.7, color=['#5975a4','#99ccff'], legend='top_right')
bar.axis[0].axis_label=''
bar.axis[1].axis_label='Train Time Ratio (XGBoost/LightGBM)'
bar.axis[1].axis_label_text_font_size='12pt'
bar.y_range = Range1d(0, 30)
bar.toolbar_location='above'
bar.legend[0].visible=True
show(bar)
In [20]:
bar.output_backend = "svg"
export_svgs(bar, filename="xgb_vs_lgbm_train_time.svg")
display(SVG('xgb_vs_lgbm_train_time.svg'))
SVG Image
Plot of train time ratio for GPU experiments.

In [21]:
data = {
    'Ratio': results_gpu['Train Time Ratio'].values.tolist() + results_gpu['Train Time Ratio Hist'].values.tolist(),
    'label': results_gpu.index.values.tolist()*2,
    'group': ['xgb/lgb']*len(results_gpu.index.values) + ['xgb_hist/lgb']*len(results_gpu.index.values)
}
In [22]:
bar = Bar(data, values='Ratio', agg='mean', label='label', group='group', 
          plot_width=600, plot_height=400, bar_width=0.5, color=['#ff8533','#ffd1b3'], legend='top_right')
bar.axis[0].axis_label=''
bar.y_range = Range1d(0, 30)
bar.axis[1].axis_label='Train Time Ratio (XGBoost/LightGBM)'
bar.axis[1].axis_label_text_font_size='12pt'
bar.toolbar_location='above'
bar.legend[0].visible=True
show(bar)
In [23]:
bar.output_backend = "svg"
export_svgs(bar, filename="xgb_vs_lgbm_train_time_gpu.svg")
display(SVG('xgb_vs_lgbm_train_time_gpu.svg'))
WARNING:bokeh.io:The webdriver raised a TimeoutException while waiting for                      a 'bokeh:idle' event to signify that the layout has rendered.                      Something may have gone wrong.
SVG Image
In [24]:
data = {
    'Perf. Difference(%)': results_df['Perf. Difference(%)'].values,
    'label': results_df.index.values
}
In [25]:
bar = Bar(data, values='Perf. Difference(%)', agg='mean', label=['label'], 
          plot_width=600, plot_height=400, bar_width=0.7, color='#5975a4')
bar.axis[0].axis_label=''
bar.axis[1].axis_label='Perf. Difference(%)'
bar.toolbar_location='above'
bar.legend[0].visible=False
show(bar)
In [26]:
bar.output_backend = "svg"
export_svgs(bar, filename="xgb_vs_lgbm_performance.svg")
display(SVG('xgb_vs_lgbm_performance.svg'))
WARNING:bokeh.io:The webdriver raised a TimeoutException while waiting for                      a 'bokeh:idle' event to signify that the layout has rendered.                      Something may have gone wrong.
SVG Image
For the speed results we can see that LightGBM is on average 5 times faster than the CPU and GPU versions of XGBoost and XGBoost histogram. In regards to the performance, we can see that LightGBM is sometimes better and sometimes worse.

Analyzing the results of XGBoost in CPU we can see that XGBoost histogram is faster than XGBoost in the Airline, Fraud and HIGGS datasets, but much slower in Planet and BCI dataset. In these two cases there is a memory overhead due to the high number of features. In the case of football dataset, the histogram implementation is slightly slower, we believe that there could be a slight principle of memory overhead.

Finally, if we look at the results of XGBoost in GPU we see that there are several values missing. This is due to an out of memory of the standard version. In our experiments we observed that XGBoost's memory consumption is around 10 times higher than LightGBM and 5 times higher than XGBoost histogram. We see that the histogram version is faster except in the BCI dataset, where there could be a memory overhead like in the CPU version.
